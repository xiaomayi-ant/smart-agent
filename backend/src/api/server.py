import asyncio
import json
import uuid
import os
from typing import Dict, Any, List
from datetime import datetime
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from ..core.config import settings
from ..core.graph import graph, GraphState, create_graph, register_stream_callback
from ..models.types import ThreadCreateResponse, StreamRequest
from ..tools.registry import TOOL_BY_NAME
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from ..store.threads_pg import (
    ensure_thread,
    insert_message as pg_insert_message,
    load_messages as pg_load_messages,
    delete_thread as pg_delete_thread,
    touch_thread as pg_touch_thread,
    get_thread_owner,
)

# Import document routes (optional)
try:
    from .document_routes import router as document_router
    DOCUMENT_ROUTES_AVAILABLE = True
except ImportError as e:
    print(f"[Server] Document routes not available: {e}")
    DOCUMENT_ROUTES_AVAILABLE = False
# Import websearch routes (optional)
try:
    from .websearch_routes import router as websearch_router
    WEBSEARCH_ROUTES_AVAILABLE = True
except ImportError as e:
    print(f"[Server] Websearch routes not available: {e}")
    WEBSEARCH_ROUTES_AVAILABLE = False

# Import ASR routes (optional, behind feature flag)
ASR_ROUTES_AVAILABLE = False
if getattr(settings, "enable_voice", False):
    try:
        from .asr_routes import router as asr_router
        ASR_ROUTES_AVAILABLE = True
    except ImportError as e:
        print(f"[Server] ASR routes not available: {e}")
        ASR_ROUTES_AVAILABLE = False


# Initialize FastAPI app
app = FastAPI(
    title="Universal Assistant API",
    description="Universal Assistant LangGraph Python implementation",
    version="0.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization", "X-Requested-With"],
)

# Auth middleware: extract user_id from Authorization: Bearer <JWT>
@app.middleware("http")
async def auth_middleware(request: Request, call_next):
    try:
        auth = request.headers.get("authorization") or request.headers.get("Authorization")
        user_id = None
        if auth and auth.lower().startswith("bearer "):
            token = auth.split(" ", 1)[1].strip()
            if token:
                try:
                    import jwt  # PyJWT
                    secret = getattr(settings, "jwt_secret", None) or ""
                    payload = jwt.decode(token, secret, algorithms=["HS256"])  # type: ignore
                    sub = payload.get("sub")
                    if isinstance(sub, str) and len(sub) > 0:
                        user_id = sub
                except Exception:
                    pass
        request.state.user_id = user_id
    except Exception:
        request.state.user_id = None
    response = await call_next(request)
    return response

# Include document routes if available
if DOCUMENT_ROUTES_AVAILABLE:
    app.include_router(document_router)
    print("[Server] Document processing routes registered")

if WEBSEARCH_ROUTES_AVAILABLE:
    app.include_router(websearch_router)
    print("[Server] Websearch routes registered")

if ASR_ROUTES_AVAILABLE and getattr(settings, "enable_voice", False):
    app.include_router(asr_router)
    print("[Server] ASR routes registered")

# Warmup intent router on startup (non-blocking if disabled)
@app.on_event("startup")
def warmup_intent_router():
    try:
        from ..intent.manager import warmup as _intent_warmup  # type: ignore
        ok = _intent_warmup()
        print(f"[startup] intent router warmup: {ok}")
    except Exception as e:
        print(f"[startup] intent warmup skipped: {e}")

# Initialize Async Postgres checkpointer at startup (optional)
@app.on_event("startup")
async def init_async_checkpointer():
    try:
        dsn = getattr(settings, "pg_dsn", None)
        if not dsn:
            return
        try:
            from ..core.auto_reconnect_checkpointer import AutoReconnectCheckpointer
            from ..core.checkpointer_adapter import MinimalCheckpointerAdapter  # æ¢å¤ä½¿ç”¨
        except Exception as e:
            print(f"[startup] AutoReconnect/Adapter import failed: {e}")
            return
        # Build an auto-reconnecting saver and pre-connect to ensure setup()
        auto = AutoReconnectCheckpointer(dsn, max_retry=3, connection_max_age=210, setup_on_connect=True)
        try:
            await auto.__aenter__()
            print("[startup] AutoReconnectCheckpointer is connected and ready")
        except Exception as e:
            print(f"[startup] AutoReconnectCheckpointer enter failed: {e}")
            return
        # Wrap with serialization adapter
        saver = MinimalCheckpointerAdapter(auto)  # ä½¿ç”¨ä¿®å¤åçš„ adapter
        # Recompile graph with async checkpointer
        global graph
        graph = create_graph().compile(checkpointer=saver)
        print("[startup] graph compiled with AutoReconnectCheckpointer + MinimalCheckpointerAdapter (fixed)")
        # Ensure proper shutdown cleanup
        async def _shutdown_cm():
            try:
                await auto.__aexit__(None, None, None)
            except Exception:
                pass
        try:
            app.add_event_handler("shutdown", _shutdown_cm)
        except Exception:
            pass
    except Exception as e:
        print(f"[startup] init_async_checkpointer failed: {e}")

# Thread history storage removed; persisted store is the source of truth


def send_sse_event(data: Dict[str, Any], event: str = "message") -> str:
    """Format SSE event"""
    return f"event: {event}\ndata: {json.dumps(data)}\n\n"


def split_token_into_chunks(token: str, max_chunk_size: int = 200) -> List[str]:
    """Split long token into smaller chunks"""
    chunks = []
    current_chunk = ""
    i = 0
    
    while i < len(token):
        current_chunk += token[i]
        i += 1
        
        if len(current_chunk) >= max_chunk_size or i == len(token):
            chunks.append(current_chunk)
            current_chunk = ""
    
    return chunks


@app.post("/api/threads", response_model=ThreadCreateResponse)
async def create_thread(request: Request):
    """Create a new thread"""
    print("[Threads] æ”¶åˆ°åˆ›å»ºçº¿ç¨‹è¯·æ±‚")
    # Require authenticated user to avoid orphan threads
    req_uid = getattr(request.state, "user_id", None)
    if not req_uid:
        raise HTTPException(status_code=401, detail="Unauthorized")

    thread_id = f"thread_{uuid.uuid4().hex[:8]}"
    print(f"[Threads] åˆ›å»ºæ–°çº¿ç¨‹: {thread_id}")
    # Persist thread skeleton
    try:
        await ensure_thread(thread_id, req_uid)
    except Exception as e:
        print(f"[Threads] ensure_thread failed: {e}")
    return ThreadCreateResponse(thread_id=thread_id)


@app.post("/api/threads/{thread_id}/runs/stream")
async def stream_response(thread_id: str, request: StreamRequest, http_request: Request):
    """Stream response endpoint"""
    print(f"[Stream] æ”¶åˆ°æµå¼è¯·æ±‚ï¼Œçº¿ç¨‹ID: {thread_id}")
    print(f"[Stream] è¾“å…¥: {request.input}")
    # æ—©è¿”å›ï¼šå°†çº¿ç¨‹æ ¡éªŒä¸å½’å±æ£€æŸ¥ç§»å…¥ç”Ÿæˆå™¨å†…éƒ¨ï¼Œå…ˆå‘ ACK å‡å°‘é¦–å­—å»¶è¿Ÿ

    # In-memory thread history removed. Persistence is used directly where needed.
    
    async def generate_stream():
        """Generate streaming response"""
        # ğŸ”¥ ç«‹å³æ‰“å°ï¼Œç¡®è®¤åç«¯ä½•æ—¶æ”¶åˆ°è¯·æ±‚
        print(f"[TIMING] ğŸ¯ åç«¯æ”¶åˆ°è¯·æ±‚ï¼æ—¶é—´: {datetime.now().isoformat()}")
        
        main_message_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
        current_content = ""
        has_streamed_content = False
        tool_calls = []
        
        # ä½¿ç”¨é˜Ÿåˆ—å®æ—¶æ¨é€äº‹ä»¶ï¼Œé¿å…ç¼“å†²å¯¼è‡´ä¸€æ¬¡æ€§è¾“å‡º
        event_queue: asyncio.Queue[Any] = asyncio.Queue()
        done_sentinel = object()
        
        # ç«‹å³å‘é€åˆå§‹äº‹ä»¶
        yield send_sse_event({
            "id": main_message_id,
            "object": "chat.completion.chunk",
            "created": int(datetime.now().timestamp()),
            "model": getattr(settings, "llm_model", "unknown"),
            "choices": [{
                "index": 0,
                "delta": {"role": "assistant"},
                "finish_reason": None
            }]
        }, "message")
        
        # ACK åå†è¿›è¡Œçº¿ç¨‹å­˜åœ¨æ€§ä¸å½’å±æ£€æŸ¥ï¼Œå¤±è´¥åˆ™å‘é€é”™è¯¯å¹¶ç»“æŸ
        try:
            try:
                await ensure_thread(thread_id, getattr(http_request.state, "user_id", None))
            except Exception as e:
                print(f"[Stream] ensure_thread failed (post-ack): {e}")
            try:
                owner = await get_thread_owner(thread_id)
                req_uid = getattr(http_request.state, "user_id", None)
                if owner is None or owner != req_uid:
                    raise HTTPException(status_code=404, detail="Thread not found")
            except HTTPException as he:
                err_msg = getattr(he, 'detail', 'Thread not found')
                yield send_sse_event({"error": err_msg, "type": "HTTPException"}, "error")
                return
            except Exception as e:
                print(f"[Stream] ownership check error (post-ack): {e}")
                yield send_sse_event({"error": "Thread not found", "type": "HTTPException"}, "error")
                return
        except Exception:
            # å®‰å…¨å…œåº•ï¼šè‹¥æ ¡éªŒæµç¨‹è‡ªèº«å¼‚å¸¸ï¼Œç›´æ¥ç»“æŸ
            return
        
        try:
            # Prepare messages
            messages = request.input.get("messages", [])
            # å®‰å…¨æ¸…æ´—ï¼šç§»é™¤æŒ‡å‘æœ¬åœ°é¢„è§ˆç«¯ç‚¹çš„ image_urlï¼Œé¿å…ä¸Šæ¸¸ä¸‹è½½å¤±è´¥
            try:
                cleaned = []
                for m in messages:
                    mm = dict(m) if isinstance(m, dict) else m
                    content = mm.get('content')
                    if isinstance(content, list):
                        new_parts = []
                        for p in content:
                            if isinstance(p, dict) and p.get('type') == 'image_url':
                                url = ((p.get('image_url') or {}) or {}).get('url') if isinstance(p.get('image_url'), dict) else p.get('image_url')
                                url_str = str(url or '')
                                if 'localhost:3000/api/preview' in url_str or '/api/preview/' in url_str:
                                    # ä¸¢å¼ƒè¯¥å›¾ç‰‡éƒ¨ä»¶
                                    continue
                            new_parts.append(p)
                        mm['content'] = new_parts
                    cleaned.append(mm)
                messages = cleaned
            except Exception:
                pass
            if not messages:
                raise HTTPException(status_code=400, detail="No messages provided")
            # Persist last user message (raw payload) if present
            try:
                for msg in reversed(messages):
                    role_or_type = (msg.get('role') or msg.get('type') or '').lower()
                    if role_or_type in ["user", "human"]:
                        await pg_insert_message(thread_id, "user", msg, getattr(http_request.state, "user_id", None))
                        break
            except Exception as e:
                print(f"[Stream] æŒä¹…åŒ–ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
            
            # Convert to LangChain messages
            lc_messages = []
            for msg in messages:
                # Handle both 'role' and 'type' fields for compatibility
                role_or_type = msg.get('role') or msg.get('type')
                content = msg.get('content', '')
                
                # å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆæ”¯æŒå›¾ç‰‡è¯†åˆ«ï¼‰
                if isinstance(content, list):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å« image_urlï¼ˆéœ€è¦å¤šæ¨¡æ€æ¨¡å‹ï¼‰
                    has_image = any(
                        isinstance(part, dict) and part.get('type') in ('image_url', 'image')
                        for part in content
                    )
                    
                    if has_image:
                        # ä¿ç•™å¤šæ¨¡æ€ç»“æ„ï¼Œç›´æ¥ä¼ ç»™ LLMï¼ˆç”¨äºè§†è§‰è¯†åˆ«ï¼‰
                        # LangChain æ”¯æŒ content ä¸º list çš„æ ¼å¼
                        processed_content = []
                        for part in content:
                            if isinstance(part, dict):
                                part_type = part.get('type', '')
                                if part_type == 'text':
                                    processed_content.append({
                                        "type": "text",
                                        "text": part.get('text', '')
                                    })
                                elif part_type == 'image_url':
                                    # ä¿ç•™ image_url ç»“æ„
                                    processed_content.append({
                                        "type": "image_url",
                                        "image_url": part.get('image_url', {})
                                    })
                                elif part_type == 'image':
                                    # æ”¯æŒå‰ç«¯ç›´æ¥ä¼  image å­—æ®µï¼ˆè½¬ä¸º image_url æ ¼å¼ï¼‰
                                    image_data = part.get('image', '')
                                    if image_data:
                                        processed_content.append({
                                            "type": "image_url",
                                            "image_url": {"url": image_data}
                                        })
                                elif part_type == 'file':
                                    # æ–‡ä»¶ç±»å‹æš‚æ—¶è½¬ä¸ºæ–‡æœ¬è¯´æ˜
                                    processed_content.append({
                                        "type": "text",
                                        "text": f"[æ–‡ä»¶: {part.get('name', '')} ({part.get('contentType', '')})]"
                                    })
                                else:
                                    # å…¶ä»–æœªçŸ¥ç±»å‹ä¹Ÿè½¬ä¸ºæ–‡æœ¬
                                    processed_content.append({
                                        "type": "text",
                                        "text": f"[{part_type} å†…å®¹]"
                                    })
                        content = processed_content
                    else:
                        # æ²¡æœ‰å›¾ç‰‡ï¼ŒæŒ‰åŸé€»è¾‘è½¬ä¸ºçº¯æ–‡æœ¬
                        combined_content = []
                        for part in content:
                            if isinstance(part, dict):
                                if part.get('type') == 'text':
                                    combined_content.append(part.get('text', ''))
                                elif part.get('type') == 'file':
                                    combined_content.append(f"[æ–‡ä»¶: {part.get('name', '')} ({part.get('contentType', '')})]")
                                else:
                                    combined_content.append(f"[{part.get('type', '')} å†…å®¹]")
                            else:
                                combined_content.append(str(part))
                        content = "\n\n".join(combined_content)
                
                if role_or_type in ["user", "human"]:
                    lc_messages.append(HumanMessage(content=content))
                elif role_or_type in ["assistant", "ai"]:
                    lc_messages.append(AIMessage(content=content))
                elif role_or_type == "system":
                    lc_messages.append(SystemMessage(content=content))
            
            # å›¾ç‰‡é¢„å¤„ç†ï¼šå…ˆè¯†åˆ«å›¾ç‰‡å†…å®¹ï¼Œè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œå†è¿›å…¥æ­£å¸¸æµç¨‹
            try:
                if getattr(settings, "enable_vision", True):
                    # æ£€æµ‹æ˜¯å¦åŒ…å«å›¾ç‰‡
                    has_vision_content = False
                    vision_message_index = -1
                    
                    for idx, msg in enumerate(lc_messages):
                        if isinstance(msg.content, list):
                            has_image = any(
                                isinstance(p, dict) and p.get('type') in ('image_url', 'image')
                                for p in msg.content
                            )
                            if has_image:
                                has_vision_content = True
                                vision_message_index = idx
                                break
                    
                    if has_vision_content and vision_message_index >= 0:
                        print("[Vision] æ£€æµ‹åˆ°å›¾ç‰‡å†…å®¹ï¼Œå¼€å§‹è§†è§‰è¯†åˆ«é¢„å¤„ç†")
                        
                        from ..core.config import get_chat_llm
                        vision_llm = get_chat_llm(temperature=0.1)
                        
                        msg = lc_messages[vision_message_index]
                        content_parts = msg.content if isinstance(msg.content, list) else []
                        
                        # æå–ç”¨æˆ·åŸå§‹é—®é¢˜
                        text_parts = [p.get('text', '') for p in content_parts if isinstance(p, dict) and p.get('type') == 'text']
                        original_question = text_parts[0] if text_parts else 'è¯·æè¿°è¿™å¼ å›¾ç‰‡'
                        
                        # æå–å›¾ç‰‡éƒ¨åˆ†
                        image_parts = [p for p in content_parts if isinstance(p, dict) and p.get('type') in ('image_url', 'image')]
                        
                        print(f"[Vision] æå–åˆ°çš„å›¾ç‰‡éƒ¨åˆ†: {len(image_parts)} ä¸ª")
                        for idx, img in enumerate(image_parts):
                            print(f"[Vision]   å›¾ç‰‡ {idx+1}: type={img.get('type')}, keys={list(img.keys())}, æ•°æ®é•¿åº¦={len(str(img))}")
                        
                        # ğŸ”‘ è½¬æ¢å›¾ç‰‡æ ¼å¼ä¸º OpenAI æœŸæœ›çš„æ ¼å¼
                        normalized_image_parts = []
                        for img in image_parts:
                            if img.get('type') == 'image' and 'image' in img:
                                # å‰ç«¯æ ¼å¼ï¼š{'type': 'image', 'image': 'data:image/png;base64,...'}
                                # è½¬æ¢ä¸º OpenAI æ ¼å¼ï¼š{'type': 'image_url', 'image_url': {'url': '...'}}
                                image_data = img['image']
                                
                                # ğŸ› è¯¦ç»†è°ƒè¯•
                                print(f"[Vision]   ğŸ” åŸå§‹å›¾ç‰‡æ•°æ®:")
                                print(f"[Vision]     - æ•°æ®ç±»å‹: {type(image_data)}")
                                print(f"[Vision]     - æ•°æ®é•¿åº¦: {len(image_data) if isinstance(image_data, str) else 'N/A'}")
                                print(f"[Vision]     - å‰100å­—ç¬¦: {image_data[:100] if isinstance(image_data, str) else 'N/A'}")
                                print(f"[Vision]     - æ˜¯å¦ data URI: {image_data.startswith('data:') if isinstance(image_data, str) else False}")
                                
                                normalized_image_parts.append({
                                    "type": "image_url",
                                    "image_url": {"url": image_data}
                                })
                                print(f"[Vision]   âœ… è½¬æ¢æ ¼å¼: image -> image_url")
                            elif img.get('type') == 'image_url':
                                # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
                                image_url = img.get('image_url', {})
                                url = image_url.get('url', '') if isinstance(image_url, dict) else ''
                                
                                # ğŸ› è¯¦ç»†è°ƒè¯•
                                print(f"[Vision]   ğŸ” image_url æ•°æ®:")
                                print(f"[Vision]     - URL ç±»å‹: {type(url)}")
                                print(f"[Vision]     - URL é•¿åº¦: {len(url) if isinstance(url, str) else 'N/A'}")
                                print(f"[Vision]     - URL å‰100å­—ç¬¦: {url[:100] if isinstance(url, str) else 'N/A'}")
                                
                                normalized_image_parts.append(img)
                                print(f"[Vision]   âœ… æ ¼å¼æ­£ç¡®: image_url")
                        
                        # æ„é€ è§†è§‰è¯†åˆ«æ¶ˆæ¯
                        # ğŸ› æµ‹è¯•ï¼šä½¿ç”¨æ›´ç®€å•ç›´æ¥çš„è‹±æ–‡æç¤ºè¯ï¼ˆGPT å¯¹è‹±æ–‡å“åº”æ›´å¥½ï¼‰
                        vision_content = [
                            {"type": "text", "text": "What's in this image? Describe everything you can see."},
                        ]
                        vision_content.extend(normalized_image_parts)
                        
                        vision_message = HumanMessage(content=vision_content)
                        
                        # ğŸ› æ‰“å°æœ€ç»ˆå‘é€çš„æ¶ˆæ¯ç»“æ„
                        print("[Vision] ğŸ“¤ å‘é€ç»™ OpenAI çš„æ¶ˆæ¯ç»“æ„:")
                        print(f"[Vision]   - æ¶ˆæ¯ç±»å‹: {type(vision_message)}")
                        print(f"[Vision]   - content ç±»å‹: {type(vision_message.content)}")
                        print(f"[Vision]   - content é•¿åº¦: {len(vision_message.content) if isinstance(vision_message.content, list) else 'N/A'}")
                        if isinstance(vision_message.content, list):
                            for idx, part in enumerate(vision_message.content):
                                print(f"[Vision]   - Part {idx+1}: type={part.get('type') if isinstance(part, dict) else type(part)}")
                                if isinstance(part, dict) and part.get('type') == 'image_url':
                                    url = part.get('image_url', {}).get('url', '') if isinstance(part.get('image_url'), dict) else ''
                                    print(f"[Vision]     - URL å‰ç¼€: {url[:50] if url else 'empty'}")
                        
                        # è°ƒç”¨è§†è§‰ LLM è¯†åˆ«å›¾ç‰‡
                        print("[Vision] ğŸš€ æ­£åœ¨è°ƒç”¨ OpenAI API...")
                        vision_result = await vision_llm.ainvoke([vision_message])
                        print(f"[Vision] âœ… API è°ƒç”¨å®Œæˆï¼Œå“åº”ç±»å‹: {type(vision_result)}")
                        image_description = vision_result.content
                        
                        print(f"[Vision] å›¾ç‰‡è¯†åˆ«å®Œæˆ: {image_description[:150]}...")
                        
                        # å°†å›¾ç‰‡æ¶ˆæ¯æ›¿æ¢ä¸ºæ–‡æœ¬æè¿°
                        # æ ¼å¼ï¼š[å›¾ç‰‡å†…å®¹] + ç”¨æˆ·é—®é¢˜
                        new_text_content = f"[ç”¨æˆ·ä¸Šä¼ äº†ä¸€å¼ å›¾ç‰‡ï¼Œå›¾ç‰‡å†…å®¹æè¿°å¦‚ä¸‹]\n{image_description}\n\n[ç”¨æˆ·çš„é—®é¢˜]\n{original_question}"
                        
                        # æ›¿æ¢åŸæ¶ˆæ¯
                        lc_messages[vision_message_index] = HumanMessage(content=new_text_content)
                        
                        print("[Vision] å›¾ç‰‡å·²è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œè¿›å…¥æ­£å¸¸å¤„ç†æµç¨‹")
            
            except Exception as e:
                print(f"[Vision] å›¾ç‰‡é¢„å¤„ç†å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹æ¶ˆæ¯: {e}")
                import traceback
                traceback.print_exc()
            
            # Create graph state
            state = GraphState()
            state.messages = lc_messages
            
            # å°†äº‹ä»¶æ¨å…¥é˜Ÿåˆ—çš„å¸®åŠ©å‡½æ•°ï¼ˆåŒæ­¥å›è°ƒå¯å®‰å…¨è°ƒç”¨ï¼‰
            def push_event(data: Dict[str, Any], event: str = "message"):
                try:
                    event_queue.put_nowait(send_sse_event(data, event))
                except Exception:
                    pass
            
            # Stream callback function
            def stream_callback(token: str, calls: List[Any] = None, event_type: str = None):
                nonlocal current_content, has_streamed_content, tool_calls
                # è½»é‡è§‚æµ‹ï¼šä»…æ‰“å°å‰ä¸‰ä¸ª partial_ai chunk çš„é¢„è§ˆ
                try:
                    if getattr(settings, "debug", False) and token:
                        # ç»Ÿè®¡ï¼šåŸºäºå½“å‰ç´¯è®¡å†…å®¹ç²—ç•¥ä¼°ç®— chunk åºå·
                        approx_chunk_index = max(1, len(current_content) // max(len(token), 1))
                        if approx_chunk_index <= 3:
                            preview = token if len(token) <= 120 else token[:120] + "..."
                            print(f"[SSE] partial_ai idx~{approx_chunk_index} len={len(token)} preview={preview}")
                except Exception:
                    pass
                
                if calls:
                    tool_calls.extend(calls)
                
                if token:
                    current_content += token
                    has_streamed_content = True
                    
                    # Split token into chunks for smoother streaming
                    chunks = split_token_into_chunks(token)
                    for chunk in chunks:
                        # ä½¿ç”¨ partial_ai äº‹ä»¶ï¼Œå‘é€ç´¯è®¡å†…å®¹
                        payload = [{
                            "id": main_message_id,
                            "type": "ai",
                            "content": current_content,
                            "delta": chunk,
                            "tool_calls": calls or tool_calls,
                        }]
                        push_event(payload, "partial_ai")
                
                # å¤„ç†ç‰¹æ®Šäº‹ä»¶ç±»å‹
                if event_type == "on_tool_end":
                    push_event({
                        "message": "å·¥å…·æ‰§è¡Œå®Œæˆ",
                        "timestamp": int(datetime.now().timestamp())
                    }, "on_tool_end")
                elif event_type == "tool_result":
                    push_event([{
                        "type": "tool",
                        "id": f"tool-{uuid.uuid4().hex[:8]}",
                        "content": token,
                        "tool_calls": calls or []
                    }], "tool_result")
                elif event_type == "approval_required":
                    # è½¬å‘å®¡æ‰¹äº‹ä»¶ï¼Œå‰ç«¯æ®æ­¤æ˜¾ç¤ºç¡®è®¤UI
                    push_event({
                        "thread_id": thread_id,
                        "tool_calls": calls or []
                    }, "approval_required")
            
            # æ³¨å†Œçº¿ç¨‹çº§å›è°ƒï¼ˆä¸å†™å…¥ stateï¼Œé¿å…åºåˆ—åŒ–ï¼‰
            try:
                register_stream_callback(thread_id, stream_callback)
            except Exception:
                pass
            
            # å¹¶å‘æ‰§è¡Œå›¾ï¼šç”Ÿäº§è€…æŠŠäº‹ä»¶æ¨å…¥é˜Ÿåˆ—ï¼Œå½“å‰ç”Ÿæˆå™¨æ¶ˆè´¹å¹¶yield
            async def run_graph_and_finalize():
                try:
                    print(f"[Stream] å¼€å§‹æ‰§è¡Œå›¾å·¥ä½œæµ")
                    # Pass thread_id to checkpointer via config, and into state for callback registry
                    config = {"configurable": {"thread_id": thread_id}}
                    state_payload = state.to_dict()
                    try:
                        state_payload["thread_id"] = thread_id
                    except Exception:
                        pass
                    # æ³¨å…¥ user_id åˆ°å›¾çš„ stateï¼Œä¾¿äºå­å›¾ï¼ˆå¦‚ KGï¼‰ä¼ é€’ä¸Šä¸‹æ–‡
                    try:
                        req_uid = getattr(http_request.state, "user_id", None)
                        if req_uid:
                            state_payload["user_id"] = req_uid
                    except Exception:
                        pass
                    # è°ƒè¯•æ¨¡å¼ï¼šè¾“å‡ºé€èŠ‚ç‚¹äº‹ä»¶ï¼›ç”Ÿäº§é»˜è®¤ä»ç”¨ ainvoke
                    result = None
                    if os.getenv("DEBUG_GRAPH_EVENTS", "0") == "1":
                        try:
                            try:
                                print("[Phase] debug_events start")
                                push_event({"phase": "debug_events", "status": "start"}, "phase")
                            except Exception:
                                pass
                            async for ev in graph.astream_events(state_payload, config=config, stream_mode="values"):
                                try:
                                    evt = ev.get("event") or ev.get("type") or "unknown"
                                    # äº‹ä»¶å¯¹è±¡ä¸ä¸€å®šæ˜¯ dictï¼ˆå¯èƒ½æ˜¯ list/str/å…¶ä»–ï¼‰ï¼Œåšç±»å‹å®ˆå«
                                    if isinstance(ev, dict):
                                        payload = {k: v for k, v in ev.items() if k not in ()}
                                    else:
                                        payload = {"data": ev}
                                    # ç›´æ¥é€ä¼ åŸå§‹äº‹ä»¶ï¼ˆä»…è°ƒè¯•ç”¨ï¼‰
                                    push_event(payload, "debug")
                                    # LangGraph é€šå¸¸åœ¨ on_end äº‹ä»¶é‡Œæºå¸¦æœ€ç»ˆè¾“å‡º
                                    if evt == "on_end" and isinstance(ev, dict):
                                        data = ev.get("data") or {}
                                        # å¸¸è§ç»“æ„ï¼š{"output": result}
                                        result = data.get("output", data) or None
                                except Exception:
                                    pass
                            try:
                                push_event({"phase": "debug_events", "status": "end"}, "phase")
                                print("[Phase] debug_events end")
                            except Exception:
                                pass
                        except Exception as _e:
                            print(f"[Stream] debug events unavailable: {_e}")
                    if result is None:
                        try:
                            push_event({"phase": "formal", "status": "start"}, "phase")
                            print("[Phase] formal start")
                        except Exception:
                            pass
                        result = await graph.ainvoke(state_payload, config=config)
                    
                    # Final event for OpenAI-style finish
                    if has_streamed_content:
                        push_event({
                            "id": main_message_id,
                            "object": "chat.completion.chunk",
                            "created": int(datetime.now().timestamp()),
                            "model": getattr(settings, "llm_model", "unknown"),
                            "choices": [{
                                "index": 0,
                                "delta": {},
                                "finish_reason": "stop"
                            }]
                        }, "message")
                        # æ”¶å°¾æ‘˜è¦ï¼šä»…åœ¨ debug æ—¶æ‰“å°ä¸€æ¬¡
                        try:
                            if getattr(settings, "debug", False):
                                last_preview = current_content[-120:] if len(current_content) > 120 else current_content
                                print(f"[SSE] partial_ai done total_len={len(current_content)} last_preview={last_preview}")
                        except Exception:
                            pass
                    
                    # å‘é€ complete äº‹ä»¶ï¼ˆå¯¹è±¡è½½è·ï¼Œé¿å…å‰ç«¯æŒ‰â€œæ¶ˆæ¯æ•°ç»„â€è§£æï¼‰
                    push_event({
                        "id": main_message_id,
                        "type": "complete",
                        "created": int(datetime.now().timestamp())
                    }, "complete")
                    # é˜¶æ®µç»“æŸæ ‡è®°ï¼ˆformalï¼‰
                    try:
                        push_event({"phase": "formal", "status": "end"}, "phase")
                        print("[Phase] formal end")
                    except Exception:
                        pass
                    
                    # In-memory history update removed
                    # Persist assistant final message and touch thread
                    try:
                        if has_streamed_content:
                            # Use accumulated content if available
                            payload = {"type": "text", "text": current_content}
                            await pg_insert_message(thread_id, "assistant", payload, getattr(http_request.state, "user_id", None))
                            # Optional verbose logging of final content for cross-checking with frontend
                            try:
                                if os.getenv("LOG_FULL_ASSISTANT_REPLY", "1") == "1":
                                    print(f"[Stream] Final content length: {len(current_content)}")
                                    print(f"[Stream] Final content: {current_content}")
                            except Exception:
                                pass
                        else:
                            # å…œåº•ï¼šæ— æµå¼å†…å®¹æ—¶ï¼Œå°è¯•ä» result ä¸­æå–æ–‡æœ¬
                            try:
                                text_out = ""
                                if isinstance(result, dict):
                                    text_out = (result.get("final_answer") or "")
                                    if not text_out:
                                        msgs = result.get("messages") or []
                                        if isinstance(msgs, list) and len(msgs) > 0:
                                            last = msgs[-1]
                                            try:
                                                text_out = getattr(last, 'content', '') or ''
                                            except Exception:
                                                text_out = str(last)
                                if text_out:
                                    if os.getenv("LOG_FULL_ASSISTANT_REPLY", "1") == "1":
                                        preview = text_out if len(text_out) <= 500 else text_out[:500] + "..."
                                        print(f"[Stream] Fallback content length: {len(text_out)}")
                                        print(f"[Stream] Fallback content preview: {preview}")
                                    await pg_insert_message(thread_id, "assistant", {"type": "text", "text": text_out}, getattr(http_request.state, "user_id", None))
                                else:
                                    print("[Stream] No streamed content and no text found in result; skip persist")
                            except Exception as _e_fb:
                                print(f"[Stream] fallback persist failed: {_e_fb}")
                        await pg_touch_thread(thread_id, getattr(http_request.state, "user_id", None))
                    except Exception as e:
                        print(f"[Stream] æŒä¹…åŒ–åŠ©æ‰‹æ¶ˆæ¯/æ›´æ–°çº¿ç¨‹å¤±è´¥: {e}")

                    # æ­£å¸¸å®Œæˆï¼šé€šçŸ¥æ¶ˆè´¹å¾ªç¯ç»“æŸ
                    try:
                        await event_queue.put(done_sentinel)
                        print("[Stream] queued done_sentinel (normal)")
                    except Exception:
                        pass
                
                except Exception as e:
                    import traceback
                    err_type = type(e).__name__
                    err_msg = str(e)
                    err_tb = traceback.format_exc()
                    print(f"[Stream] é”™è¯¯: {err_type}: {err_msg}\n{err_tb}")
                    try:
                        push_event({"error": err_msg, "type": err_type, "trace": err_tb}, "error")
                    except Exception:
                        pass
                    finally:
                        # å¼‚å¸¸æ—¶ä¹Ÿç¡®ä¿é€šçŸ¥æ¶ˆè´¹å¾ªç¯ç»“æŸ
                        try:
                            await event_queue.put(done_sentinel)
                        except Exception:
                            pass
            
            producer_task = asyncio.create_task(run_graph_and_finalize())
            
            # æ¶ˆè´¹å¹¶å®æ—¶è¿”å›äº‹ä»¶
            while True:
                event_or_sentinel = await event_queue.get()
                if event_or_sentinel is done_sentinel:
                    break
                yield event_or_sentinel
            
            # ç¡®ä¿ç”Ÿäº§è€…ç»“æŸ
            try:
                await producer_task
            finally:
                pass
            print(f"[Stream] æµå¼å“åº”å®Œæˆ")
        except Exception as e:
            import traceback
            err_type = type(e).__name__
            err_msg = str(e)
            err_tb = traceback.format_exc()
            print(f"[Stream] é”™è¯¯: {err_type}: {err_msg}\n{err_tb}")
            yield send_sse_event({
                "error": err_msg,
                "type": err_type,
                "trace": err_tb
            }, "error")
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@app.get("/api/threads/{thread_id}/messages")
async def get_thread_messages(thread_id: str, request: Request):
    """Get thread messages"""
    try:
        # Prefer persisted store for auth; in-memory is best-effort and not per-user
        rows = await pg_load_messages(thread_id, getattr(request.state, "user_id", None))
        if rows is None:
            raise HTTPException(status_code=404, detail="Thread not found")
        return {
            "thread_id": thread_id,
            "messages": rows,
        }
    except HTTPException:
        raise
    except Exception as e:
        print(f"[Threads] load messages error: {e}")
        raise HTTPException(status_code=500, detail="Failed to load messages")


@app.delete("/api/threads/{thread_id}")
async def delete_thread(thread_id: str, request: Request):
    """Delete a thread"""
    try:
        await pg_delete_thread(thread_id, getattr(request.state, "user_id", None))
        # In-memory cache cleanup removed
        return {"message": "Thread deleted successfully"}
    except Exception as e:
        print(f"[Threads] åˆ é™¤çº¿ç¨‹å¤±è´¥: {e}")
        raise HTTPException(status_code=404, detail="Thread not found")


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Universal Assistant API",
        "version": "0.0.0",
        "status": "running"
    }


@app.post("/api/threads/{thread_id}/tools/approval")
async def approve_tool(thread_id: str, request: Request):
    """Approve or reject a tool execution and optionally execute it.

    Body: { toolName: str, args: dict, approve: bool, toolCallId?: str }
    """
    try:
        body = await request.json()
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid JSON body")

    tool_name = (body or {}).get("toolName")
    args = (body or {}).get("args") or {}
    approve = bool((body or {}).get("approve"))
    tool_call_id = (body or {}).get("toolCallId")

    # Ownership check (same as stream)
    try:
        owner = await get_thread_owner(thread_id)
        req_uid = getattr(request.state, "user_id", None)
        if owner is None or owner != req_uid:
            raise HTTPException(status_code=404, detail="Thread not found")
    except HTTPException:
        raise
    except Exception as e:
        print(f"[approval] ownership check error: {e}")
        raise HTTPException(status_code=404, detail="Thread not found")

    if not tool_name:
        raise HTTPException(status_code=400, detail="toolName is required")

    # Persist the decision first
    try:
        await pg_insert_message(thread_id, "assistant", {
            "type": "approval_result",
            "approve": approve,
            "toolName": tool_name,
            "args": args,
            **({"toolCallId": tool_call_id} if tool_call_id else {}),
        }, getattr(request.state, "user_id", None))
    except Exception as e:
        print(f"[approval] persist decision failed: {e}")

    if not approve:
        return {"ok": True}

    # Execute tool on approval
    tool = TOOL_BY_NAME.get(tool_name)
    if not tool:
        raise HTTPException(status_code=400, detail=f"Unknown tool: {tool_name}")

    try:
        result = await tool.ainvoke(args)
        try:
            await pg_insert_message(thread_id, "assistant", {
                "type": "tool_result",
                "toolName": tool_name,
                "args": args,
                "result": result,
            }, getattr(request.state, "user_id", None))
            await pg_touch_thread(thread_id, getattr(request.state, "user_id", None))
        except Exception as e:
            print(f"[approval] persist result failed: {e}")
        return {"ok": True, "result": result}
    except Exception as e:
        print(f"[approval] tool execution error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "src.api.server:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug
    ) 